# Embeddings Explorer (PoC)

Measure how strongly **GPTâ€‘4oâ€¯mini** links a brand to the words, phrases and competitors that people normally mention â€“ without needing any internal model embeddings.

---

## 1â€‚Why this existsÂ  ğŸ¤”

Large language models donâ€™t expose their hidden vector space, but they **do** let us ask for perâ€‘token logâ€‘probabilities (`logprobs`).  
If a phrase is *much* more probable after the prompt â€œA word I associate with **McDonaldâ€™s** is â†’â€ than after the neutral prompt â€œA word I associate with ____ is â†’â€, the model is signalling that it sees a tight association.  
OpenAIâ€™s GPTâ€‘4â€‘family (incl. **GPTâ€‘4oÂ mini**) surfaces those probabilities via a simple boolean flag.  [oai_citation:0â€¡OpenAI Cookbook](https://cookbook.openai.com/examples/using_logprobs?utm_source=chatgpt.com)

This PoC turns that signal into a repeatable number (`Î”â€¯logâ€¯P`) and a ranked list you can track over time.

---

## 2â€‚How it worksÂ  âš™ï¸

| Stage | Script | What happens |
|-------|--------|--------------|
|**Discovery**|`discover.py`|â€¢ **Topâ€‘token scan** â€“ grab the 20 mostâ€‘likely next tokens and keep ones that are brandâ€‘skewed.<br>â€¢ **Iterative extension** â€“ grow each token into a short phrase while the skew stays â‰¥â€¯0.5.<br>â€¢ **Highâ€‘T list sampling** â€“ ask GPTâ€‘4oâ€¯mini for five associated words/phrases 100Ã— at Tâ€¯=â€¯0.8.<br>â€¢ **Competitor sampling** â€“ same trick but â€œcompanies that compete withâ€¦â€.<br>â€¢ **Manual injection** â€“ any lines in `manual_phrases.txt` are forced into the pool.|
|**Scoring**|`score.py` (coming next)|For each candidate phrase: 1) calculate logâ€¯P with the brand prompt, 2) logâ€¯P with the neutral prompt, 3) store the delta (`Î”â€¯logâ€¯P`).|
|**Dashboard**|`dash.py` (optional)|Streamlit frontâ€‘end that shows a leagueâ€‘table and monthâ€‘overâ€‘month sparkâ€‘lines.|

---

## 3â€‚Project treeÂ  ğŸŒ³
â”œâ”€â”€ discover.py
â”œâ”€â”€ score.py
â”œâ”€â”€ dash.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ phrases.csv      # discovered terms (this repo autoâ€‘generates)
â”‚   â””â”€â”€ scores.duckdb    # scored results (autoâ€‘generates)
â”œâ”€â”€ .env                 # OPENAI_API_KEY=skâ€‘â€¦
â””â”€â”€ README.md

---

## 4â€‚Quick startÂ  ğŸš€

```bash
git clone https://github.com/yourâ€‘org/brandâ€‘probe.git
cd brandâ€‘probe
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt            # openai, pythonâ€‘dotenv, tiktoken, etc.

echo 'OPENAI_API_KEY=skâ€‘â€¦' > .env          # or export in your shell

python discover.py --brand "McDonald's"    # fills data/phrases.csv
python score.py    --brand "McDonald's"    # computes Î”â€¯logâ€¯P
streamlit run dash.py                      # visualise
```

## 5â€‚Interpreting Î”â€¯logâ€¯PÂ  ğŸ“
	â€¢	**Î”â€¯>â€¯1â€¯**â€‚â‰ˆâ€¯eÂ²Â·â· times more likely with the brand prompt â€“ strong association
	â€¢	0â€¯<â€¯Î”â€¯â‰¤â€¯1â€‚â€“ moderate link
	â€¢	Î”â€¯â‰¤â€¯0â€‚â€“ no meaningful link (or even a negative one)

Because both prompts share the same surface structure, subtracting them cancels generic word frequency and isolates the brand effect.

---

## 6â€‚Limitations & gotchasÂ  âš ï¸
	â€¢	Tokenisation quirks â€“ smart quotes, apostrophes and ampersands change token IDs; the scripts normalise but doubleâ€‘check edge cases.
	â€¢	API drift â€“ OpenAI occasionally tweaks field names; the code uses the current logprobs.content[..].top_logprobs schema (Mayâ€¯2025).
	â€¢	Logprob variance â€“ scores fluctuate a bit between runs; we recommend averaging three passes for dashboards.
	â€¢	Modelâ€‘specific view â€“ GPTâ€‘4oâ€¯mini â‰  Claude â‰  Gemini. Run the same pipeline on other providers to spot blind spots.

---

## 7â€‚Roadâ€‘mapÂ  ğŸ—ºï¸
	1.	score.py â€“ batch scorer with DuckDB storage.
	2.	Crossâ€‘model panel â€“ sideâ€‘byâ€‘side GPTâ€‘4oâ€¯mini vs. Mistralâ€¯Large.
	3.	Sentiment split â€“ add a second prompt (â€œPeople feel thatâ€¯â€¦â€) to track opinion separately from knowledge.